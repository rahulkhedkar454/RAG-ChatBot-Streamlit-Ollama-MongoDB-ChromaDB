import atexit
import base64
import json
import os
import re
import time
import uuid
from datetime import datetime
from typing import Iterator

import chromadb
import pandas as pd
import plotly.express as px

# Show statistics
import psutil
import pynvml
import requests
import streamlit as st
import tiktoken
from chromadb.utils import embedding_functions
from dotenv import dotenv_values
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure
from ratelimit import limits, sleep_and_retry
from tenacity import retry, stop_after_attempt, wait_exponential

# import pkg_resources


config = dotenv_values(".env")
# Configuration
MODEL_NAME = config["MODEL_NAME"]
OLLAMA_API_URL = config["OLLAMA_API_URL"]
MONGODB_URI = config["MONGODB_URI"]
DATABASE_NAME = config["DATABASE_NAME"]
COLLECTION_NAME = config["COLLECTION_NAME"]
CHROMCHADB_COLLECTION_NAME = config["CHROMCHADB_COLLECTION_NAME"]
CHROMADB_PERSIST_DIR = config["CHROMADB_PERSIST_DIR"]

MAX_CONTEXT_LENGTH = int(config["MAX_CONTEXT_LENGTH"])
CONTEXT_WINDOW = int(config["CONTEXT_WINDOW"])
SUMMARIZE_THRESHOLD = int(config["SUMMARIZE_THRESHOLD"])
CALLS = int(config["CALLS"])
PERIOD = int(config["PERIOD"])


# Custom Ollama Embedding Function
class OllamaEmbeddingFunction:
    def __init__(self, model_name: str = "nomic-embed-text"):
        self.model_name = model_name
        self.api_url = f"{OLLAMA_API_URL}/api/embeddings"

    def __call__(self, texts: list) -> list:
        embeddings = []
        for text in texts:
            try:
                response = requests.post(
                    self.api_url,
                    json={"model": self.model_name, "prompt": text},
                    timeout=10,
                )
                if response.status_code == 200:
                    embeddings.append(response.json()["embedding"])
                else:
                    st.warning(f"Failed to generate embedding for text: {text[:50]}...")
                    embeddings.append([])
            except Exception as e:
                st.warning(f"Embedding generation failed: {str(e)}")
                embeddings.append([])
        return embeddings


# Initialize ChromaDB client with persistent storage
@st.cache_resource
def init_chromadb():
    """Initialize ChromaDB persistent client and collection."""
    try:
        os.makedirs(CHROMADB_PERSIST_DIR, exist_ok=True)
        client = chromadb.PersistentClient(path=CHROMADB_PERSIST_DIR)
        try:
            # Try using built-in OllamaEmbeddingFunction if available
            embedding_function = embedding_functions.OllamaEmbeddingFunction(
                model_name="nomic-embed-text", url=OLLAMA_API_URL
            )
        except AttributeError:
            # Fallback to custom embedding function
            embedding_function = OllamaEmbeddingFunction(model_name="nomic-embed-text")

        collection = client.get_or_create_collection(
            name=CHROMCHADB_COLLECTION_NAME, embedding_function=embedding_function
        )
        return collection
    except Exception as e:
        st.error(f"‚ùå Failed to initialize ChromaDB: {str(e)}")
        return None


collection = init_chromadb()


# MongoDB connection
@st.cache_resource
def init_mongodb():
    """Initialize MongoDB connection and return collection and client."""
    try:
        client = MongoClient(MONGODB_URI)
        client.admin.command("ping")
        db = client[DATABASE_NAME]
        collection = db[COLLECTION_NAME]
        return collection, client
    except ConnectionFailure:
        st.error("‚ùå MongoDB connection failed. Please ensure MongoDB is running.")
        return None, None


# def cleanup_mongodb():
#     """Close MongoDB connection."""
#     try:
#         if 'mongo_client' in st.session_state and st.session_state.mongo_client:
#             st.session_state.mongo_client.close()
#             del st.session_state.mongo_client
#     except Exception as e:
#         st.warning(f"Failed to cleanup MongoDB: {str(e)}")

# Register MongoDB cleanup on process exit
# atexit.register(cleanup_mongodb)

mongo_collection, mongo_client = init_mongodb()
if mongo_client:
    st.session_state.mongo_client = mongo_client

# Page configuration
st.set_page_config(
    page_title="NoCapGenAI",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Custom CSS
st.markdown(
    """
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        text-align: center;
        color: white;
    }
    .chat-container {
        background-color: #f8f9fa;
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .user-message {
        background-color: #e3f2fd;
        border-left: 4px solid #2196f3;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 5px;
    }
    .assistant-message {
        background-color: #f3e5f5;
        border-left: 4px solid #9c27b0;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 5px;
    }
    .code-block {
        background-color: #2d3748;
        color: #e2e8f0;
        padding: 1rem;
        border-radius: 5px;
        margin: 0.5rem 0;
        font-family: 'Courier New', monospace;
    }
</style>
""",
    unsafe_allow_html=True,
)

# Prompt templates
PROMPT_TEMPLATES = {
    "python_dev": """You are an expert Python developer and coding assistant. 
    You maintain context across our conversation and can reference previous code and discussions.
    Focus on writing clean, efficient, and well-documented Python code.
    Provide best practices, error handling, and optimization suggestions.
    
    Previous conversation context:
    {context}
    
    Current user question: {query}""",
    "ml_engineer": """You are an expert Machine Learning Engineer and Data Scientist.
    You remember our previous discussions about ML models, data, and experiments.
    Help with ML algorithms, data preprocessing, model training, evaluation, and deployment.
    Include code examples with popular libraries like scikit-learn, pandas, numpy, tensorflow, pytorch.
    
    Previous conversation context:
    {context}
    
    Current user question: {query}""",
    "mongodb_expert": """You are a MongoDB expert specializing in database design, queries, and optimization.
    You remember our previous discussions about database, queries, and performance.
    Provide efficient MongoDB queries, schema design advice, and performance optimization tips.
    Include Python code using pymongo when relevant.
    
    Previous conversation context:
    {context}
    
    Current user question: {query}""",
    "code_review": """You are a senior code reviewer with context of our ongoing code review session.
    You remember previous code snippets and review feedback we've discussed.
    Analyze the following code for:
    - Best practices
    - Performance optimization
    - Security issues
    - Code readability
    - Potential bugs
    
    Previous conversation context:
    {context}
    
    Code/Question: {query}""",
    "debug_helper": """You are a debugging expert who remembers our debugging session.
    You can reference previous error messages, code attempts, and solutions we've tried.
    Provide step-by-step debugging approach and suggest solutions.
    
    Previous conversation context:
    {context}
    
    Code/Error: {query}""",
    "general": """You are a helpful, knowledgeable, and creative general-purpose AI assistant.
    You can answer questions, provide explanations, help with coding, solve problems, brainstorm ideas, and assist with a wide range of topics including technology, science, history, language, and more.
    You maintain context of our conversation and can reference previous topics and code.
    Provide clear, practical, and insightful responses.
    
    Previous conversation context:
    {context}
    
    Current user question: {query}""",
}


# Utility functions
def sanitize_input(text: str) -> str:
    """Sanitize user input to prevent injection attacks."""
    return re.sub(r"[<>;{}]", "", text.strip())


def generate_embedding(text: str) -> list:
    """Generate text embedding using Ollama API."""
    try:
        response = requests.post(
            f"{OLLAMA_API_URL}/api/embeddings",
            json={"model": "nomic-embed-text", "prompt": text},
            timeout=10,
        )
        if response.status_code != 200:
            raise Exception(f"Error generating embedding: {response.text}")
        return response.json()["embedding"]
    except Exception as e:
        st.warning(f"Failed to generate embedding: {str(e)}")
        return []


def save_embedding_to_vector_db(text: str, session_id: str, metadata: dict = {}):
    """Save text and its embedding to ChromaDB."""
    if collection is None:
        st.error("‚ùå ChromaDB not initialized.")
        return
    try:
        vector = generate_embedding(text)
        if vector:
            collection.add(
                documents=[text],
                embeddings=[vector],
                ids=[f"{session_id}_{time.time()}"],
                metadatas=[metadata],
            )
    except Exception as e:
        st.warning(f"Failed to save to vector DB: {str(e)}")


def check_ollama_status() -> bool:
    """Check if Ollama server is running."""
    try:
        response = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=5)
        return response.status_code == 200
    except requests.RequestException:
        return False


def get_available_models() -> list:
    """Get list of available Ollama models."""
    try:
        response = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            return [model["name"] for model in models]
        return []
    except requests.RequestException:
        return []


@sleep_and_retry
@limits(calls=CALLS, period=PERIOD)
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def call_ollama_stream(prompt: str, model: str = MODEL_NAME) -> Iterator[str]:
    """Calls the Ollama API with streaming enabled to generate responses incrementally.

    Args:
        prompt (str): The input prompt for the model.
        model (str): The name of the model to use (default: MODEL_NAME).

    Yields:
        str: Incremental response chunks from the model.

    Returns:
        str: The complete response string.
    """
    try:
        prompt = sanitize_input(prompt)
        payload = {"model": model, "prompt": prompt, "stream": True}

        response = requests.post(
            f"{OLLAMA_API_URL}/api/generate", json=payload, stream=True, timeout=300
        )

        if response.status_code == 200:
            full_response = ""
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line.decode("utf-8"))
                        if "response" in data:
                            full_response += data["response"]
                            yield data["response"]
                        if data.get("done", False):
                            break
                    except json.JSONDecodeError:
                        continue
            return full_response
        else:
            error_msg = f"Error: HTTP {response.status_code}"
            yield error_msg
            return error_msg

    except requests.RequestException as e:
        error_msg = f"Error connecting to Ollama: {str(e)}"
        yield error_msg
        return error_msg


def call_ollama(prompt: str, model: str = MODEL_NAME) -> str:
    """Call Ollama API without streaming."""
    try:
        prompt = sanitize_input(prompt)
        payload = {"model": model, "prompt": prompt, "stream": False}

        response = requests.post(
            f"{OLLAMA_API_URL}/api/generate", json=payload, timeout=120
        )

        if response.status_code == 200:
            return response.json().get("response", "No response received")
        return f"Error: HTTP {response.status_code} - {response.text}"

    except requests.RequestException as e:
        return f"Error connecting to Ollama: {str(e)}"


def estimate_tokens(text: str) -> int:
    """Estimate token count for text using tiktoken."""
    try:
        encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
        return len(encoding.encode(text))
    except:
        return len(text) // 4


def build_context(messages: list, max_length: int = MAX_CONTEXT_LENGTH) -> str:
    """Build context from recent messages, staying within token limit."""
    if not messages:
        return ""

    recent_messages = messages[-CONTEXT_WINDOW:]
    context_parts = []
    current_length = 0

    for message in reversed(recent_messages):
        role = message["role"]
        content = message["content"]

        formatted_msg = f"{'User' if role == 'user' else 'Assistant'}: {content}"
        msg_tokens = estimate_tokens(formatted_msg)

        if current_length + msg_tokens > max_length:
            break

        context_parts.insert(0, formatted_msg)
        current_length += msg_tokens

    return "\n\n".join(context_parts)


def summarize_conversation(messages: list, model: str = MODEL_NAME) -> list:
    """Summarize older parts of conversation to maintain context."""
    if len(messages) < SUMMARIZE_THRESHOLD:
        return messages

    recent_messages = messages[-CONTEXT_WINDOW:]
    older_messages = messages[:-CONTEXT_WINDOW]

    summary_text = ""
    for msg in older_messages:
        if msg["role"] == "user":
            summary_text += f"User asked: {msg['content'][:100]}...\n"
        else:
            summary_text += f"Assistant provided: {msg['content'][:100]}...\n"

    summary_prompt = f"""Please provide a concise summary of this conversation focusing on:
    - Key topics discussed
    - Code examples shared
    - Problems solved
    - Important context to remember
    
    Conversation to summarize:
    {summary_text}
    
    Summary:"""

    try:
        summary = call_ollama(summary_prompt, model)
        summary_message = {
            "role": "assistant",
            "content": f"[CONVERSATION SUMMARY: {summary}]",
            "is_summary": True,
        }
        return [summary_message] + recent_messages
    except:
        return recent_messages


def save_chat_to_db(
    user_message: str, assistant_response: str, prompt_type: str, session_id: str = None
):
    """Save chat to MongoDB."""
    if mongo_collection is not None:
        try:
            if "session_id" not in st.session_state:
                st.session_state.session_id = str(uuid.uuid4())
            session_id = st.session_state.session_id

            is_new_session = not mongo_collection.find_one({"session_id": session_id})
            title = (
                generate_session_title(st.session_state.messages)
                if is_new_session
                else None
            )

            chat_entry = {
                "timestamp": datetime.now(),
                "session_id": session_id,
                "user_message": user_message,
                "assistant_response": assistant_response,
                "prompt_type": prompt_type,
                "model": MODEL_NAME,
                "message_count": len(st.session_state.messages),
            }

            if title:
                chat_entry["title"] = title

            mongo_collection.insert_one(chat_entry)
        except Exception as e:
            st.error(f"Failed to save to database: {str(e)}")


def retrieve_similar_context(query: str, top_k: int = 3) -> list:
    """Retrieve similar context from ChromaDB."""
    if collection is None:
        st.error("‚ùå ChromaDB not initialized.")
        return []
    try:
        query_vector = generate_embedding(query)
        if query_vector:
            results = collection.query(query_embeddings=[query_vector], n_results=top_k)
            return results["documents"][0] if results and results["documents"] else []
        return []
    except Exception as e:
        st.warning(f"Memory retrieval failed: {e}")
        return []


def load_session_history(session_id: str, limit: int = 50) -> list:
    """Load chat history for a session."""
    if mongo_collection is not None:
        try:
            chats = list(
                mongo_collection.find({"session_id": session_id})
                .sort("timestamp", 1)
                .limit(limit)
            )
            return chats
        except Exception as e:
            st.error(f"Failed to load session history: {str(e)}")
            return []
    return []


def get_recent_conversations(limit: int = 10) -> list:
    """Get recent conversation sessions."""
    if mongo_collection is not None:
        try:
            pipeline = [
                {"$sort": {"timestamp": -1}},
                {
                    "$group": {
                        "_id": "$session_id",
                        "latest_timestamp": {"$first": "$timestamp"},
                        "message_count": {"$sum": 1},
                        "first_message": {"$last": "$user_message"},
                    }
                },
                {"$sort": {"latest_timestamp": -1}},
                {"$limit": limit},
            ]
            sessions = list(mongo_collection.aggregate(pipeline))
            return sessions
        except Exception as e:
            st.error(f"Failed to get recent conversations: {str(e)}")
            return []
    return []


def load_all_sessions_with_messages() -> list:
    """Load all sessions with their messages."""
    if mongo_collection is None:
        return []

    try:
        pipeline = [
            {"$sort": {"timestamp": -1}},
            {
                "$group": {
                    "_id": "$session_id",
                    "latest_timestamp": {"$first": "$timestamp"},
                }
            },
            {"$sort": {"latest_timestamp": -1}},
        ]
        sorted_session_ids = [
            doc["_id"] for doc in mongo_collection.aggregate(pipeline)
        ]
        all_sessions = []

        for session_id in sorted_session_ids:
            session_messages = list(
                mongo_collection.find({"session_id": session_id}).sort("timestamp", 1)
            )

            summary = None
            title = None
            messages = []

            for chat in session_messages:
                if chat.get("is_summary"):
                    summary = chat.get("content")
                elif chat.get("title") and not title:
                    title = chat.get("title")
                elif chat.get("user_message") and chat.get("assistant_response"):
                    messages.append(
                        {
                            "user": chat.get("user_message", ""),
                            "assistant": chat.get("assistant_response", ""),
                        }
                    )

            # Handle sessions with a single message
            if not messages and session_messages:
                # If no user-assistant pair but session exists, create a minimal message
                chat = session_messages[0]
                if chat.get("user_message"):
                    messages.append(
                        {
                            "user": chat.get("user_message", ""),
                            "assistant": chat.get(
                                "assistant_response", "No response yet"
                            ),
                        }
                    )

            # Generate title if none exists
            if not title:
                title = (
                    generate_session_title(messages) if messages else "Untitled Session"
                )

            all_sessions.append(
                {
                    "session_id": session_id,
                    "title": title.strip(),
                    "summary": summary,
                    "messages": messages,
                    "total_messages": len(messages),
                    "timestamp": (
                        session_messages[0]["timestamp"] if session_messages else None
                    ),
                }
            )

        return all_sessions
    except Exception as e:
        st.error(f"Failed to load sessions: {str(e)}")
        return []


def generate_session_title(messages: list, model: str = MODEL_NAME) -> str:
    """Generate a short, meaningful title for the session."""
    if not messages:
        return "Untitled Session"

    # Use the first user message if available
    context_snippet = ""
    for m in messages[:3]:
        if isinstance(m, dict) and m.get("role") == "user":
            context_snippet = m["content"][:100]
            break
        elif isinstance(m, dict) and m.get("user"):
            context_snippet = m["user"][:100]
            break

    if not context_snippet:
        return "Untitled Session"

    prompt = f"""
    Based on the following conversation, generate a short and descriptive title (max 10 words):
    User: {context_snippet}...
    Return only the title, no explanation or additional text.
    Title:"""

    try:
        title = call_ollama(prompt, model)
        # Extract only the title (remove any explanation or extra text)
        title = title.strip().replace('"', "")
        # Sanitize title: remove newlines, special characters, and limit length
        title = re.sub(r'[\n\r\t<>:"/\\|?*]', "", title)
        title = title[:50]  # Limit to 50 characters to avoid long filenames
        return title if title else context_snippet[:50]
    except:
        return context_snippet[:50] if context_snippet else "Untitled Session"


def get_chat_stats() -> tuple:
    """Get chat statistics."""
    if mongo_collection is not None:
        try:
            total_chats = mongo_collection.count_documents({})
            prompt_types = mongo_collection.aggregate(
                [{"$group": {"_id": "$prompt_type", "count": {"$sum": 1}}}]
            )
            return total_chats, list(prompt_types)
        except Exception as e:
            st.error(f"Failed to get stats: {str(e)}")
            return 0, []
    return 0, []


def generate_markdown_export(messages: list, title: str = "Chat Session") -> str:
    """Generate Markdown export of chat session."""
    if not messages:
        return f"# {title}\n\n**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n**Model:** {MODEL_NAME}\n\n**Note:** No messages available in this session.\n"

    export_md = f"# {title}\n\n**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n**Model:** {MODEL_NAME}\n\n"
    for i, msg in enumerate(messages):
        user_text = (
            msg.get("user", msg.get("content", ""))
            if msg.get("role") == "user"
            else msg.get("user", "")
        )
        assistant_text = (
            msg.get("assistant", msg.get("content", ""))
            if msg.get("role") == "assistant"
            else msg.get("assistant", "")
        )

        if user_text:
            export_md += f"**User {i//2 + 1}:**\n{user_text}\n\n"

        if assistant_text:
            export_md += f"**Assistant:**\n{assistant_text}\n\n"
    export_md += "\n---\n\n**End of Session**\n"
    return export_md


def export_as_markdown_button(messages: list, title: str):
    """Render a button to download chat as Markdown."""
    # Sanitize title for use in filename
    safe_title = re.sub(r'[\n\r\t<>:"/\\|?*]', "", title)[:50]
    safe_title = safe_title if safe_title else "Chat_Session"

    md_content = generate_markdown_export(messages, title)
    b64 = base64.b64encode(md_content.encode()).decode()
    href = f'<a href="data:text/markdown;base64,{b64}" download="{safe_title}.md">üì• Download Chat as Markdown</a>'
    st.markdown(href, unsafe_allow_html=True)


# Sidebar
with st.sidebar:
    st.header("üîß Assistant Settings")

    ollama_status = check_ollama_status()
    if ollama_status:
        st.success("‚úÖ Ollama Server Connected")
        available_models = get_available_models()
        if available_models:
            selected_model = st.selectbox(
                "Select Model",
                available_models,
                index=(
                    0
                    if MODEL_NAME not in available_models
                    else available_models.index(MODEL_NAME)
                ),
            )
            MODEL_NAME = selected_model
        else:
            st.warning("‚ö†Ô∏è No models found. Please pull some models first.")
            st.code("ollama pull phi3:mini")
    else:
        st.error("‚ùå Ollama Server Not Running")
        st.code("ollama serve")

    use_streaming = st.checkbox("Enable Streaming Response", value=True)
    prompt_type = st.selectbox(
        "Assistant Type",
        [
            "python_dev",
            "ml_engineer",
            "mongodb_expert",
            "code_review",
            "debug_helper",
            "general",
        ],
    )

    st.header("üß† Context Management")
    if st.session_state.get("messages"):
        total_messages = len(st.session_state.messages)
        context_length = estimate_tokens(build_context(st.session_state.messages))

        col1, col2 = st.columns(2)
        with col1:
            st.metric("Messages", total_messages)
        with col2:
            st.metric("Context Tokens", context_length)

        if total_messages > SUMMARIZE_THRESHOLD:
            if st.button("üìù Summarize Conversation"):
                st.session_state.messages = summarize_conversation(
                    st.session_state.messages, MODEL_NAME
                )
                st.success("Conversation summarized to maintain context!")
                st.rerun()

        context_window = st.slider("Context Window", 5, 20, CONTEXT_WINDOW)
        if context_window != CONTEXT_WINDOW:
            st.session_state.context_window = context_window

    st.header("üí¨ Session Management")
    if st.button("üÜï New Chat Session"):
        st.session_state.show_stats = False
        st.session_state.show_history = False
        st.session_state.show_sessions_history = False

        st.session_state.messages = []
        st.session_state.session_id = str(uuid.uuid4())
        # cleanup_mongodb()  # Clean up MongoDB connection on new session
        st.success("Started new chat session!")
        st.rerun()

    recent_sessions = get_recent_conversations(5)
    if recent_sessions:
        st.subheader("Recent Sessions")
        for session in recent_sessions:
            session_label = f"{session['first_message'][:30]}... ({session['message_count']} messages)"
            if st.button(session_label, key=f"session_{session['_id']}"):
                session_history = load_session_history(session["_id"])
                st.session_state.messages = []
                for chat in session_history:
                    st.session_state.messages.append(
                        {"role": "user", "content": chat["user_message"]}
                    )
                    st.session_state.messages.append(
                        {"role": "assistant", "content": chat["assistant_response"]}
                    )
                st.session_state.session_id = session["_id"]
                st.success(f"Loaded session with {len(session_history)} messages!")
                st.rerun()

    st.header("‚ö° Quick Actions")
    if st.button("üìä View Statistics"):
        st.session_state.show_stats = True

    if st.button("üìö Chat History"):
        st.session_state.show_history = True

    if st.button("üîÑ Load All Sessions"):
        st.session_state.show_sessions_history = True

    if st.button("üóëÔ∏è Clear Current Chat"):
        st.session_state.messages = []
        # cleanup_mongodb()  # Clean up MongoDB connection on clear
        st.rerun()

    if st.button("üîÑ Refresh Models"):
        st.rerun()

    st.header("ü§ñ Model Info")
    st.info(f"Current Model: {MODEL_NAME}")
    st.info(f"Assistant Type: {prompt_type.replace('_', ' ').title()}")
    st.info(f"Streaming: {'Enabled' if use_streaming else 'Disabled'}")

    st.header("üóÑÔ∏è Database")
    if mongo_collection is not None:
        st.success("‚úÖ MongoDB Connected")
    else:
        st.error("‚ùå MongoDB Disconnected")

    st.header("üß† Vector Database")
    if collection is not None:
        st.success("‚úÖ ChromaDB Connected")
        st.info(f"Persistence Directory: {CHROMADB_PERSIST_DIR}")
        # st.info(f"ChromaDB Version: {pkg_resources.get_distribution('chromadb').version}")
    else:
        st.error("‚ùå ChromaDB Disconnected")

# Main header
st.markdown(
    """
<div class="main-header">
    <h1>ü§ñ NoCapGenAI</h1>
    <p>Fast, flexible, and fully local ‚Äî your all-in-one Generative AI assistant powered by Ollama, MongoDB, and ChromaDB.</p>
</div>
""",
    unsafe_allow_html=True,
)

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []


# Show statistics
if st.session_state.get("show_stats", False):
    st.session_state.show_sessions_history = False
    st.session_state.show_history = False

    st.header("üìä Usage Statistics")
    total_chats, prompt_types = get_chat_stats()

    # CPU Usage
    cpu_usage = psutil.cpu_percent(interval=1)

    # RAM Usage
    memory = psutil.virtual_memory()
    ram_available = memory.available / (1024**3)  # Convert to GB
    ram_total = memory.total / (1024**3)  # Convert to GB
    ram_usage_percent = memory.percent

    # GPU Usage (if available)
    gpu_usage = "N/A"
    gpu_memory_used = "N/A"
    gpu_memory_total = "N/A"
    try:
        pynvml.nvmlInit()
        device_count = pynvml.nvmlDeviceGetCount()
        if device_count > 0:
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Get first GPU
            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
            memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            gpu_usage = utilization.gpu  # GPU core utilization (%)
            gpu_memory_used = memory_info.used / (1024**3)  # Convert to GB
            gpu_memory_total = memory_info.total / (1024**3)  # Convert to GB
        pynvml.nvmlShutdown()
    except (pynvml.NVMLError, ImportError):
        pass  # GPU metrics will remain "N/A" if pynvml is not available or no GPU is detected

    # Display metrics
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Conversations", total_chats)
        st.metric("CPU Usage", f"{cpu_usage:.1f}%")
    with col2:
        st.metric("Current Model", MODEL_NAME)
        st.metric("RAM Available", f"{ram_available:.2f} GB / {ram_total:.2f} GB")
    with col3:
        st.metric(
            "Database Status",
            "Connected" if mongo_collection is not None else "Disconnected",
        )
        st.metric("GPU Usage", f"{gpu_usage}%")
        st.metric(
            "GPU Memory",
            (
                f"{gpu_memory_used} GB / {gpu_memory_total} GB"
                if gpu_memory_total != "N/A"
                else "N/A"
            ),
        )

    if prompt_types:
        df = pd.DataFrame(prompt_types)
        fig = px.pie(df, values="count", names="_id", title="Prompt Types Distribution")
        st.plotly_chart(fig, use_container_width=True)

    if st.button("Hide Statistics"):
        st.session_state.show_stats = False
        st.rerun()

# Show all sessions
# Show all sessions
if st.session_state.get("show_sessions_history", False):
    st.session_state.show_stats = False
    st.session_state.show_history = False

    st.session_state.all_sessions = load_all_sessions_with_messages()
    st.header("üìö All Chat Sessions")

    for session in st.session_state.all_sessions:
        title = session["title"]
        date = (
            session["timestamp"].strftime("%Y-%m-%d %H:%M")
            if isinstance(session["timestamp"], datetime)
            else "Unknown Date"
        )
        label = f"üìå {title} ‚Äî {date} | {session['total_messages']} message{'s' if session['total_messages'] != 1 else ''}"

        with st.expander(label):
            col1, col2 = st.columns([1, 2])
            with col1:
                if st.button(
                    "‚ñ∂Ô∏è Load this session", key=f"load_{session['session_id']}"
                ):
                    st.session_state.show_sessions_history = False
                    st.session_state.show_history = False
                    st.session_state.messages = []
                    for chat in session["messages"]:
                        st.session_state.messages.append(
                            {"role": "user", "content": chat["user"]}
                        )
                        if chat["assistant"]:
                            st.session_state.messages.append(
                                {"role": "assistant", "content": chat["assistant"]}
                            )
                    st.session_state.session_id = session["session_id"]
                    st.success(f"Session '{title}' loaded!")
                    st.rerun()

            with col2:
                export_as_markdown_button(session["messages"], session["title"])

            if session["summary"]:
                st.markdown(f"üìù **Summary:** {session['summary']}")

            for i, msg in enumerate(session["messages"]):
                if msg["user"]:
                    st.markdown(f"**User {i//2 + 1}:** {msg['user']}")
                if msg["assistant"]:
                    st.markdown(f"**Assistant:** {msg['assistant']}")

# Show chat history
if st.session_state.get("show_history", False):
    st.session_state.show_stats = False
    st.session_state.show_sessions_history = False
    st.session_state.show_history = True

    st.header("üìö Recent Chat History")
    history = load_session_history(
        st.session_state.get("session_id", str(uuid.uuid4())), 20
    )

    if history:
        for chat in history:
            timestamp_str = (
                chat["timestamp"].strftime("%Y-%m-%d %H:%M:%S")
                if isinstance(chat["timestamp"], datetime)
                else str(chat["timestamp"])
            )
            with st.expander(f"Chat from {timestamp_str}"):
                st.markdown(f"**User:** {chat['user_message'][:100]}...")
                st.markdown(f"**Assistant:** {chat['assistant_response'][:200]}...")
                st.markdown(
                    f"**Type:** {chat['prompt_type']} | **Model:** {chat['model']}"
                )
    else:
        st.info("No chat history found.")

    if st.button("Hide History"):
        st.session_state.show_history = False
        st.rerun()

# Chat interface
st.header("üí¨ Chat Interface")
for message in st.session_state.messages:

    with st.chat_message(message["role"]):
        content = message["content"]
        if message["role"] == "user":
            st.markdown(
                f'<div class="user-message">{content}</div>', unsafe_allow_html=True
            )
        else:
            if "```" in content:
                parts = content.split("```")
                for i, part in enumerate(parts):
                    if i % 2 == 1:
                        lang = (
                            part.split("\n")[0]
                            if part.split("\n")[0] in ["python", "sql", "bash"]
                            else "text"
                        )
                        st.code(
                            "\n".join(part.split("\n")[1:]) if lang != "text" else part,
                            language=lang,
                        )
                    else:
                        st.markdown(
                            f'<div class="assistant-message">{part}</div>',
                            unsafe_allow_html=True,
                        )
            else:
                st.markdown(
                    f'<div class="assistant-message">{content}</div>',
                    unsafe_allow_html=True,
                )

# Chat input
if prompt := st.chat_input("Ask me anything about Python, ML, or MongoDB..."):
    st.session_state.show_stats = False
    st.session_state.show_history = False
    st.session_state.show_sessions_history = False

    if not check_ollama_status():
        st.error("‚ùå Ollama server is not running. Please start it with `ollama serve`")
        st.stop()

    if prompt.lower().strip().startswith("#learn"):
        learn_text = prompt.replace("#learn", "").strip()
        if learn_text:
            session_id = st.session_state.get("session_id", str(uuid.uuid4()))
            st.session_state.session_id = session_id
            with st.spinner("Saving to memory..."):
                save_embedding_to_vector_db(
                    learn_text,
                    session_id=session_id,
                    metadata={
                        "source": "user_memory",
                        "timestamp": str(datetime.now()),
                    },
                )
            st.session_state.messages.append({"role": "user", "content": prompt})
            st.session_state.messages.append(
                {"role": "assistant", "content": "‚úÖ Learned and saved to memory."}
            )

            with st.chat_message("user"):
                st.markdown(
                    f'<div class="user-message">{prompt}</div>', unsafe_allow_html=True
                )
            with st.chat_message("assistant"):
                st.markdown(
                    '<div class="assistant-message">‚úÖ Learned and saved to memory.</div>',
                    unsafe_allow_html=True,
                )
        else:
            st.warning("‚ö†Ô∏è Nothing to learn. Please add text after `#learn`.")
        st.stop()

    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(f'<div class="user-message">{prompt}</div>', unsafe_allow_html=True)

    base_context = build_context(st.session_state.messages)
    memory_snippets = retrieve_similar_context(prompt, top_k=3)
    memory_context = "\n".join(f"[LEARNED MEMORY]\n{m}" for m in memory_snippets if m)
    combined_context = memory_context + "\n\n" + base_context

    formatted_prompt = PROMPT_TEMPLATES[prompt_type].format(
        context=combined_context, query=prompt
    )

    with st.chat_message("assistant"):
        if use_streaming:
            response_placeholder = st.empty()
            full_response = ""
            with st.spinner("ü§î Thinking..."):
                for chunk in call_ollama_stream(formatted_prompt, MODEL_NAME):
                    if chunk and not chunk.startswith("Error"):
                        full_response += chunk
                        response_placeholder.markdown(
                            f'<div class="assistant-message">{full_response}‚ñé</div>',
                            unsafe_allow_html=True,
                        )
                response_placeholder.markdown(
                    f'<div class="assistant-message">{full_response}</div>',
                    unsafe_allow_html=True,
                )
        else:
            with st.spinner("ü§î Thinking..."):
                full_response = call_ollama(formatted_prompt, MODEL_NAME)
                if "```" in full_response:
                    parts = full_response.split("```")
                    for i, part in enumerate(parts):
                        if i % 2 == 1:
                            lang = (
                                part.split("\n")[0]
                                if part.split("\n")[0] in ["python", "sql", "bash"]
                                else "text"
                            )
                            st.code(
                                (
                                    "\n".join(part.split("\n")[1:])
                                    if lang != "text"
                                    else part
                                ),
                                language=lang,
                            )
                        else:
                            st.markdown(
                                f'<div class="assistant-message">{part}</div>',
                                unsafe_allow_html=True,
                            )
                else:
                    st.markdown(
                        f'<div class="assistant-message">{full_response}</div>',
                        unsafe_allow_html=True,
                    )

    st.session_state.messages.append({"role": "assistant", "content": full_response})
    with st.spinner("Saving to database..."):
        save_chat_to_db(prompt, full_response, prompt_type)

# Example prompts
# Display Example Prompts and Footer only if no messages exist
if not st.session_state.messages:
    # Example prompts
    # st.header("üí° Example Prompts")
    # col1, col2 = st.columns(2)

    # with col1:
    #     st.subheader("Python Development")
    #     example_prompts = [
    #         "Create a REST API using FastAPI with authentication",
    #         "Implement a decorator for caching function results",
    #         "Write a class for handling CSV file operations",
    #         "Create a context manager for database connections"
    #     ]

    #     for example in example_prompts:
    #         if st.button(example, key=f"py_{example[:20]}"):
    #             st.session_state.messages.append({"role": "user", "content": example})
    #             st.rerun()

    # with col2:
    #     st.subheader("ML Engineering")
    #     ml_prompts = [
    #         "Create a machine learning pipeline for text classification",
    #         "Implement cross-validation for hyperparameter tuning",
    #         "Build a feature engineering pipeline for time series data",
    #         "Create a model evaluation framework with multiple metrics"
    #     ]

    #     for example in ml_prompts:
    #         if st.button(example, key=f"ml_{example[:20]}"):
    #             st.session_state.messages.append({"role": "user", "content": example})
    #             st.rerun()

    # Footer
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("**ü§ñ Built with:**")
        st.markdown("‚Ä¢ Streamlit")
        st.markdown("‚Ä¢ Ollama API")
        st.markdown("‚Ä¢ MongoDB")
        st.markdown("‚Ä¢ ChromaDB")

    with col2:
        st.markdown("**üåç Perfect for:**")
        st.markdown("‚Ä¢ Coding & Debugging")
        st.markdown("‚Ä¢ Data Science & ML")
        st.markdown("‚Ä¢ General Knowledge & Q&A")
        st.markdown("‚Ä¢ Brainstorming & Ideas")

    with col3:
        st.markdown("**‚ú® Features:**")
        st.markdown("‚Ä¢ Real-time streaming replies")
        st.markdown("‚Ä¢ Chat history & sessions")
        st.markdown("‚Ä¢ Multiple expert modes")
        st.markdown("‚Ä¢ Persistent vector memory")
        st.markdown("‚Ä¢ Beautiful, modern UI")

    st.markdown("---")
    st.markdown(
        "**Pro Tip:** Try different assistant types for specialized or general help! üí°"
    )
